# Model metadata - specifications for different LLM models
# Loaded by models/metadata.py

# Default metadata for unknown models
default:
  context_tokens: 8000
  max_output_tokens: 2048
  input_modes: [text]
  output_modes: [text]

# Models requiring max_completion_tokens instead of max_tokens
# GPT-5 and future models use the new parameter name
use_max_completion_tokens:
  - gpt-5
  - gpt-5-0806
  - gpt-5.2-2025-12-11
  - gpt-5.2
  - gpt-5-chat
  - gpt-5-mini
  - gpt-5-nano
  - o1-preview
  - o1-mini
  - o1
  - o3
  - o3-mini
  - o4-mini
  # GitHub Models variants (with openai/ prefix)
  - openai/gpt-5
  - openai/gpt-5-chat
  - openai/gpt-5-mini
  - openai/gpt-5-nano
  - openai/o1
  - openai/o1-preview
  - openai/o1-mini
  - openai/o3
  - openai/o3-mini
  - openai/o4-mini

# Model metadata structure:
# - context_tokens: Maximum input context window size in tokens
# - max_output_tokens: Maximum output tokens per generation
# - input_modes: List of supported input modes (text, image, etc.)
# - output_modes: List of supported output modes (text, function_calling, etc.)
models:
  # ==============================================================================
  # Anthropic Claude models (updated January 2026)
  # ==============================================================================
  claude-opus-4-5-20251101:  # Claude Opus 4.5 - flagship, most powerful
    context_tokens: 200000
    max_output_tokens: 65536  # Commonly 64k; supports extended outputs well
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  claude-haiku-4-5-20251001:  # Claude Haiku 4.5 - fast & efficient
    context_tokens: 200000
    max_output_tokens: 8192
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  claude-sonnet-4-5-20250929:  # Claude Sonnet 4.5 - best balance
    context_tokens: 200000  # 1M possible in beta for some Sonnet 4.x with header & tier
    max_output_tokens: 65536
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  claude-opus-4-1-20250805:  # Claude Opus 4.1 - incremental upgrade
    context_tokens: 200000
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  claude-opus-4-20250514:  # Claude Opus 4 - original frontier
    context_tokens: 200000
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  claude-sonnet-4-20250514:  # Claude Sonnet 4
    context_tokens: 200000  # 1M beta possible with header
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  claude-3-5-haiku-20241022:  # Claude 3.5 Haiku - legacy fast model
    context_tokens: 200000
    max_output_tokens: 8192
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  claude-3-haiku-20240307:  # Claude 3 Haiku - oldest/legacy
    context_tokens: 200000
    max_output_tokens: 4096
    input_modes: [text, image]
    output_modes: [text, function_calling]

  # ==============================================================================
  # OpenAI models (direct API - updated January 2026, active/non-deprecated only)
  # ==============================================================================
  gpt-5.2-2025-12-11:  # Latest flagship GPT-5.2 snapshot - most capable overall
    context_tokens: 400000
    max_output_tokens: 128000
    input_modes: [text, image, audio]
    output_modes: [text, function_calling]
  
  gpt-5.2:  # Alias for latest GPT-5.2
    context_tokens: 400000
    max_output_tokens: 128000
    input_modes: [text, image, audio]
    output_modes: [text, function_calling]
  
  gpt-5.2-pro:  # Pro variant - even smarter/precise, higher reasoning effort
    context_tokens: 400000
    max_output_tokens: 128000
    input_modes: [text, image, audio]
    output_modes: [text, function_calling]
  
  gpt-4.1:  # Strong non-reasoning model, huge context for docs/code
    context_tokens: 1000000
    max_output_tokens: 65536  # Practical high limit; supports long outputs
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  gpt-4.1-mini:  # Fast/cheaper version of 4.1
    context_tokens: 1000000
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  gpt-4o:  # Still excellent multimodal - your "gpt-o4", balanced & fast
    context_tokens: 128000
    max_output_tokens: 16384
    input_modes: [text, image, audio]
    output_modes: [text, function_calling]
  
  gpt-4o-mini:  # Cheap/fast multimodal alternative
    context_tokens: 128000
    max_output_tokens: 16384
    input_modes: [text, image, audio]
    output_modes: [text, function_calling]
  
  o3:  # Top reasoning model - great for math/science/coding
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  o4-mini:  # Fast reasoning - strong value for complex tasks
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]

  # ==============================================================================
  # GitHub Models API - OpenAI GPT-4 series
  # ==============================================================================
  openai/gpt-4o:
    context_tokens: 131072
    max_output_tokens: 16384
    input_modes: [text, image, audio]
    output_modes: [text, function_calling]
  
  openai/gpt-4o-mini:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text, image, audio]
    output_modes: [text, function_calling]
  
  openai/gpt-4.1:
    context_tokens: 1048576  # 1M tokens
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  openai/gpt-4.1-mini:
    context_tokens: 1048576  # 1M tokens
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  openai/gpt-4.1-nano:
    context_tokens: 1048576  # 1M tokens
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]

  # GitHub Models API - OpenAI GPT-5 series
  openai/gpt-5:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  openai/gpt-5-chat:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  openai/gpt-5-mini:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  openai/gpt-5-nano:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]

  # GitHub Models API - OpenAI o-series (reasoning models)
  openai/o1:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  openai/o1-mini:
    context_tokens: 128000
    max_output_tokens: 65536
    input_modes: [text]
    output_modes: [text, function_calling]
  
  openai/o1-preview:
    context_tokens: 128000
    max_output_tokens: 32768
    input_modes: [text]
    output_modes: [text]
  
  openai/o3:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  openai/o3-mini:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text]
    output_modes: [text, function_calling]
  
  openai/o4-mini:
    context_tokens: 200000
    max_output_tokens: 100000
    input_modes: [text, image]
    output_modes: [text, function_calling]

  # GitHub Models API - OpenAI Embeddings
  openai/text-embedding-3-large:
    context_tokens: 8191
    max_output_tokens: 3072  # Embedding dimensions
    input_modes: [text]
    output_modes: [embeddings]
  
  openai/text-embedding-3-small:
    context_tokens: 8191
    max_output_tokens: 1536  # Embedding dimensions
    input_modes: [text]
    output_modes: [embeddings]

  # ==============================================================================
  # GitHub Models API - Meta Llama series
  # ==============================================================================
  meta/llama-3.2-11b-vision-instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text, image, audio]
    output_modes: [text]
  
  meta/llama-3.2-90b-vision-instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text, image, audio]
    output_modes: [text]
  
  meta/llama-3.3-70b-instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  meta/llama-4-maverick-17b-128e-instruct-fp8:
    context_tokens: 1048576  # 1M tokens!
    max_output_tokens: 8192
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  meta/llama-4-scout-17b-16e-instruct:
    context_tokens: 1048576
    max_output_tokens: 8192
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  meta/meta-llama-3.1-405b-instruct:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  meta/meta-llama-3.1-8b-instruct:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]

  # ==============================================================================
  # GitHub Models API - Mistral AI series
  # ==============================================================================
  mistral-ai/codestral-2501:
    context_tokens: 256000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  mistral-ai/ministral-3b:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text, function_calling]
  
  mistral-ai/mistral-medium-2505:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  mistral-ai/mistral-small-2503:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text, image]
    output_modes: [text, function_calling]

  # ==============================================================================
  # GitHub Models API - DeepSeek series
  # ==============================================================================
  deepseek/deepseek-r1:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text, function_calling]
  
  deepseek/deepseek-r1-0528:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text, function_calling]
  
  deepseek/deepseek-v3-0324:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text, function_calling]

  # ==============================================================================
  # GitHub Models API - Microsoft Phi series
  # ==============================================================================
  microsoft/phi-4:
    context_tokens: 16384
    max_output_tokens: 16384
    input_modes: [text]
    output_modes: [text]
  
  microsoft/phi-4-mini-instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  microsoft/phi-4-mini-reasoning:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  microsoft/phi-4-multimodal-instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [audio, image, text]
    output_modes: [text]
  
  microsoft/phi-4-reasoning:
    context_tokens: 32768
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  microsoft/mai-ds-r1:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]

  # ==============================================================================
  # GitHub Models API - Cohere series
  # ==============================================================================
  cohere/cohere-command-a:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  cohere/cohere-command-r-08-2024:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  cohere/cohere-command-r-plus-08-2024:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text, function_calling]

  # ==============================================================================
  # GitHub Models API - AI21 Labs
  # ==============================================================================
  ai21-labs/ai21-jamba-1.5-large:
    context_tokens: 262144
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text, function_calling]

  # ==============================================================================
  # GitHub Models API - xAI Grok series
  # ==============================================================================
  xai/grok-3:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  xai/grok-3-mini:
    context_tokens: 131072
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]

  # ==============================================================================
  # Meta LLaMA models (HuggingFace direct API)
  # ==============================================================================
  meta-llama/Llama-3.1-70B-Instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  meta-llama/Llama-3.1-8B-Instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  meta-llama/Llama-3.3-70B-Instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]

  # ==============================================================================
  # xAI Grok models (direct API)
  # ==============================================================================
  grok-4-1-fast-reasoning:  # Top-tier agentic/reasoning beast
    context_tokens: 2000000
    max_output_tokens: 32768  # Practical high limit; can go higher in tests
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  grok-4-fast-reasoning:  # Cost-efficient reasoning version
    context_tokens: 2000000
    max_output_tokens: 32768
    input_modes: [text, image]
    output_modes: [text, function_calling]
  
  grok-code-fast-1:  # Specialized for coding/agents
    context_tokens: 131072  # More conservative for speed
    max_output_tokens: 16384
    input_modes: [text]  # No vision mentioned for this one
    output_modes: [text, function_calling]
  
  grok-2-vision-1212:  # Vision understanding (analyze images/charts)
    context_tokens: 131072
    max_output_tokens: 8192
    input_modes: [text, image]
    output_modes: [text]  # No tool calling specified
  
  grok-2-image-1212:  # Image generation model
    context_tokens: 4096  # Prompt-only, small context
    max_output_tokens: 1   # Special: generates images, not text tokens
    input_modes: [text]
    output_modes: [image]

  # ==============================================================================
  # Dashscope/Qwen models
  # ==============================================================================
  Qwen/Qwen3-VL-235B-A22B-Thinking-25700:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text, image]
    output_modes: [text]
  
  Qwen/Qwen3-Coder-480B-A35B-Instruct:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text]
  
  Qwen/Qwen3-VL-235B-A22B-Thinking:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text, image]
    output_modes: [text]
  
  Qwen/Qwen-Image:
    context_tokens: 4096
    max_output_tokens: 1  # Generates images
    input_modes: [text]
    output_modes: [image]
  
  Qwen/Qwen-Image-Edit-2511:
    context_tokens: 4096
    max_output_tokens: 1  # Edits images
    input_modes: [text, image]
    output_modes: [image]

  # ==============================================================================
  # DeepSeek models (direct API)
  # ==============================================================================
  deepseek-ai/DeepSeek-V3.2:
    context_tokens: 128000
    max_output_tokens: 4096
    input_modes: [text]
    output_modes: [text, function_calling]
